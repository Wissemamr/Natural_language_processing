{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 01 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.5'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model\n",
    "model = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.lang.en.English'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Google is planning to purchase a U.S. software company for $120 million.\"\n",
    "result = model(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Google is planning to purchase a U.S. software company for $120 million."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "print(type(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google | PROPN | nsubj |Google | False | False\n",
      "is | AUX | aux |be | True | False\n",
      "planning | VERB | ROOT |plan | False | False\n",
      "to | PART | aux |to | True | False\n",
      "purchase | VERB | xcomp |purchase | False | False\n",
      "a | DET | det |a | True | False\n",
      "U.S. | PROPN | compound |U.S. | False | False\n",
      "software | NOUN | compound |software | False | False\n",
      "company | NOUN | dobj |company | False | False\n",
      "for | ADP | prep |for | True | False\n",
      "$ | SYM | quantmod |$ | False | False\n",
      "120 | NUM | compound |120 | False | False\n",
      "million | NUM | pobj |million | False | False\n",
      ". | PUNCT | punct |. | False | True\n"
     ]
    }
   ],
   "source": [
    "for token in result:\n",
    "    print(\n",
    "        f\"{token.text} | {token.pos_} | {token.dep_} |{token.lemma_} | {token.is_stop} | {token.is_punct}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2) Five different properties of tokens that can be accessed using spaCy\n",
    "\n",
    "`text`: The original text of the token.\n",
    "\n",
    "Example: token.text returns \"Google\", \"is\", \"planning\", etc.\n",
    "\n",
    "`pos_` : The part-of-speech tag of the token.\n",
    "\n",
    "Example: token.pos_ might return \"NOUN\" for nouns, \"VERB\" for verbs.\n",
    "\n",
    "`dep_`: The syntactic dependency relation of the token (i.e., how the token relates to other tokens).\n",
    "\n",
    "Example: token.dep_ might return \"nsubj\" for a subject or \"dobj\" for a direct object.\n",
    "\n",
    "`lemma_`: The base form of the word.\n",
    "\n",
    "Example: token.lemma_ for the token \"planning\" returns \"plan\".\n",
    "\n",
    "`is_stop`: A boolean indicating whether the token is a stop word (e.g., \"the\", \"is\", \"and\").\n",
    "\n",
    "Example: token.is_stop returns True for common stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3) How does spaCy handle special cases in tokenization, such as punctuation, numbers, and abbreviations?\n",
    "\n",
    "`Punctuation:` SpaCy treats punctuation as separate tokens. For example, a period, comma, or quotation mark is treated as its own token.\n",
    "\n",
    "`Numbers:` Numbers are treated as single tokens. For example, \"$120\" would be tokenized as \"$\" and \"120\".\n",
    "\n",
    "`Abbreviations`: SpaCy usually handles abbreviations like \"U.S.\" correctly by keeping them as a single token instead of splitting them into multiple tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4) How does spaCy's tokenization differ from simple string splitting? Provide an example to illustrate the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text splitting : 12 tokens\n",
      "['Google', 'is', 'planning', 'to', 'purchase', 'a', 'U.S.', 'software', 'company', 'for', '$120', 'million.']\n",
      "====================\n",
      "Using Spacy : 14 tokens\n",
      "Google \n",
      "is \n",
      "planning \n",
      "to \n",
      "purchase \n",
      "a \n",
      "U.S. \n",
      "software \n",
      "company \n",
      "for \n",
      "$ \n",
      "120 \n",
      "million \n",
      ". \n"
     ]
    }
   ],
   "source": [
    "tokens1 = sent.split()\n",
    "tokens2 = model(sent)\n",
    "print(f\"Text splitting : {len(tokens1)} tokens\")\n",
    "print(tokens1)\n",
    "print(\"====================\")\n",
    "print(f\"Using Spacy : {len(tokens2)} tokens\")\n",
    "for token in tokens2:\n",
    "    print(f\"{token.text} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy splits the punctuation marks (e.g., the period after \"million\" and the \"$\" symbol) into separate tokens, while simple string splitting doesn't.\n",
    "Abbreviations like \"U.S.\" are handled better in spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5) Do the tokenization this time with word_tokenize from NLTK, what are the differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Google', 'is', 'planning', 'to', 'purchase', 'an', 'U.S.', 'software', 'company', 'for', '$', '120', 'million', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Google is planning to purchase an U.S. software company for $120 million.\"\n",
    "nltk_tokens = word_tokenize(text)\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both NLTK and spaCy split punctuation marks and numbers similarly in this case.\n",
    "The primary difference is that spaCy provides richer linguistic context (e.g., part-of-speech, dependency parsing), while NLTKâ€™s word_tokenize only splits the text into tokens without offering any additional linguistic analysis.\n",
    "SpaCy also handles special cases more robustly in many scenarios (e.g., handling of multi-word proper nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 02 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. Did he mind? Adam Jones Jr. thinks he didn't. In any case, this isn't true... Well, with a probability of .9 it isn't.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Sentence Segmentation:\n",
      "Number of Sentences: 1\n",
      "Sentence 1: Google is planning to purchase an U.S. software company for $120 million.\n",
      "\n",
      "\n",
      "NLTK Sentence Segmentation:\n",
      "Number of Sentences: 1\n",
      "Sentence 1: Google is planning to purchase an U.S. software company for $120 million.\n",
      "\n",
      "\n",
      "TextBlob Sentence Segmentation:\n",
      "Number of Sentences: 1\n",
      "Sentence 1: Google is planning to purchase an U.S. software company for $120 million.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "spacy_sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "print(\"spaCy Sentence Segmentation:\")\n",
    "print(f\"Number of Sentences: {len(spacy_sentences)}\")\n",
    "for idx, sent in enumerate(spacy_sentences):\n",
    "    print(f\"Sentence {idx+1}: {sent}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# 2. NLTK Approach\n",
    "nltk_sentences = sent_tokenize(text)\n",
    "\n",
    "print(\"NLTK Sentence Segmentation:\")\n",
    "print(f\"Number of Sentences: {len(nltk_sentences)}\")\n",
    "for idx, sent in enumerate(nltk_sentences):\n",
    "    print(f\"Sentence {idx+1}: {sent}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3. TextBlob Approach\n",
    "blob = TextBlob(text)\n",
    "textblob_sentences = [str(sentence) for sentence in blob.sentences]\n",
    "\n",
    "print(\"TextBlob Sentence Segmentation:\")\n",
    "print(f\"Number of Sentences: {len(textblob_sentences)}\")\n",
    "for idx, sent in enumerate(textblob_sentences):\n",
    "    print(f\"Sentence {idx+1}: {sent}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/wissem/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download(\"punkt\")\n",
    "text = \"Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. Did he mind? Adam Jones Jr. thinks he didn't. In any case, this isn't true... Well, with a probability of .9 it isn't.\"\n",
    "doc = nlp(text)\n",
    "spacy_sentences = [sent.text for sent in doc.sents]\n",
    "nltk_sentences = nltk.sent_tokenize(text)\n",
    "blob = TextBlob(text)\n",
    "textblob_sentences = [str(sentence) for sentence in blob.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Sentence Segmentation:\n",
      "- Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it.\n",
      "- Did he mind?\n",
      "- Adam Jones Jr. thinks he didn't.\n",
      "- In any case, this isn't true...\n",
      "- Well, with a probability of .9 it isn't.\n",
      "\n",
      "NLTK Sentence Segmentation:\n",
      "- Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e.\n",
      "- he paid a lot for it.\n",
      "- Did he mind?\n",
      "- Adam Jones Jr. thinks he didn't.\n",
      "- In any case, this isn't true... Well, with a probability of .9 it isn't.\n",
      "\n",
      "TextBlob Sentence Segmentation:\n",
      "- Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e.\n",
      "- he paid a lot for it.\n",
      "- Did he mind?\n",
      "- Adam Jones Jr. thinks he didn't.\n",
      "- In any case, this isn't true... Well, with a probability of .9 it isn't.\n"
     ]
    }
   ],
   "source": [
    "print(\"spaCy Sentence Segmentation:\")\n",
    "for sentence in spacy_sentences:\n",
    "    print(f\"- {sentence}\")\n",
    "\n",
    "print(\"\\nNLTK Sentence Segmentation:\")\n",
    "for sentence in nltk_sentences:\n",
    "    print(f\"- {sentence}\")\n",
    "\n",
    "print(\"\\nTextBlob Sentence Segmentation:\")\n",
    "for sentence in textblob_sentences:\n",
    "    print(f\"- {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Spacy` gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2) Compare the results of sentence segmentation from spaCy, NLTK, and TextBlob. Are there any differences in how they handle abbreviations, ellipsis, or other special cases?\n",
    "Comparison:\n",
    "\n",
    "spaCy: Handles abbreviations and ellipses well. For example, in the text, \"i.e.\" is correctly treated as part of the sentence and not as a sentence boundary. Similarly, ellipses like \"isn't true...\" do not trigger sentence splitting.\n",
    "NLTK: Sometimes struggles with abbreviations. It might split sentences after abbreviations like \"i.e.\" because it sees the period as an end-of-sentence marker.\n",
    "TextBlob: Similar to spaCy, it handles abbreviations and ellipses relatively well. It does not split sentences after \"i.e.\" and also manages ellipses like \"isn't true...\" without creating sentence boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 03 : Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = (\n",
    "    \"The NLP system accurately classified 95% of the customer feedback as positive.\"\n",
    ")\n",
    "doc = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: The | POS: DET | Detailed Tag: DT | Lemma: the\n",
      "Token: NLP | POS: PROPN | Detailed Tag: NNP | Lemma: NLP\n",
      "Token: system | POS: NOUN | Detailed Tag: NN | Lemma: system\n",
      "Token: accurately | POS: ADV | Detailed Tag: RB | Lemma: accurately\n",
      "Token: classified | POS: VERB | Detailed Tag: VBD | Lemma: classify\n",
      "Token: 95 | POS: NUM | Detailed Tag: CD | Lemma: 95\n",
      "Token: % | POS: NOUN | Detailed Tag: NN | Lemma: %\n",
      "Token: of | POS: ADP | Detailed Tag: IN | Lemma: of\n",
      "Token: the | POS: DET | Detailed Tag: DT | Lemma: the\n",
      "Token: customer | POS: NOUN | Detailed Tag: NN | Lemma: customer\n",
      "Token: feedback | POS: NOUN | Detailed Tag: NN | Lemma: feedback\n",
      "Token: as | POS: ADP | Detailed Tag: IN | Lemma: as\n",
      "Token: positive | POS: ADJ | Detailed Tag: JJ | Lemma: positive\n",
      "Token: . | POS: PUNCT | Detailed Tag: . | Lemma: .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(\n",
    "        f\"Token: {token.text} | POS: {token.pos_} | Detailed Tag: {token.tag_} | Lemma: {token.lemma_}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2) Different POS Tags in spaCy and Their Representation:\n",
    "- `DET (Determiner):`\n",
    "Example: \"The\" â€“ Refers to a word that introduces a noun (e.g., articles like 'the', 'a', etc.).\n",
    "\n",
    "- `PROPN (Proper Noun):`\n",
    "Example: \"NLP\" â€“ A proper noun is a specific name of a person, place, or entity.\n",
    "\n",
    "- `NOUN (Noun):`\n",
    "Example: \"system\" â€“ Refers to a general noun, not specific to a proper name.\n",
    "\n",
    "- `ADV (Adverb):`\n",
    "Example: \"accurately\" â€“ Modifies or gives additional information about a verb, adjective, or other adverbs.\n",
    "\n",
    "- `VERB (Verb):`\n",
    "Example: \"classified\" â€“ An action word that represents a state, action, or occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3) Handling Multi-word Expressions and Abbreviations in POS Tagging:\n",
    "Multi-word expressions: SpaCy treats abbreviations and multi-word expressions like \"NLP\" or \"95%\" as single tokens. For example, \"NLP\" will be tagged as a PROPN (proper noun) and \"95%\" will be tagged as NUM (numeral), indicating numerical information. SpaCy uses dependency parsing to understand the relationships between these tokens and other words in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/wissem/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/wissem/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 14 tokens\n",
      "token : The | pos_tag : DT \n",
      "token : NLP | pos_tag : NNP \n",
      "token : system | pos_tag : NN \n",
      "token : accurately | pos_tag : RB \n",
      "token : classified | pos_tag : VBD \n",
      "token : 95 | pos_tag : CD \n",
      "token : % | pos_tag : NN \n",
      "token : of | pos_tag : IN \n",
      "token : the | pos_tag : DT \n",
      "token : customer | pos_tag : NN \n",
      "token : feedback | pos_tag : NN \n",
      "token : as | pos_tag : IN \n",
      "token : positive | pos_tag : JJ \n",
      "token : . | pos_tag : . \n"
     ]
    }
   ],
   "source": [
    "sentence = (\n",
    "    \"The NLP system accurately classified 95% of the customer feedback as positive.\"\n",
    ")\n",
    "tokens = word_tokenize(sentence)\n",
    "print(f\"We have {len(tokens)} tokens\")\n",
    "nltk_pos_tags = nltk.pos_tag(tokens)\n",
    "for token, tag in nltk_pos_tags:\n",
    "    print(f\"token : {token} | pos_tag : {tag} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences between spaCy and NLTK:\n",
    "\n",
    "- spaCy: More detailed and richer tagging system with both simple POS tags (like NOUN, VERB) and fine-grained tags (like NN, VBD).\n",
    "- NLTK: Uses the Penn Treebank POS tags (e.g., NN for nouns, VB for verbs), which are more limited compared to spaCyâ€™s tagging system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 : [Stemming, Lemmatization, Name Entity Recognition, Stop words] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/wissem/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/wissem/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/wissem/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/wissem/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/wissem/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"maxent_ne_chunker\")\n",
    "nltk.download(\"words\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Apple is looking at buying a U.K. startup for $1 billion. John is excited about the new venture.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized words: ['Apple', 'be', 'look', 'at', 'buy', 'a', 'U.K.', 'startup', 'for', '$', '1', 'billion', '.', 'John', 'be', 'excited', 'about', 'the', 'new', 'venture', '.']\n"
     ]
    }
   ],
   "source": [
    "# using spacy\n",
    "doc = nlp(text)\n",
    "lemmatized_words = [token.lemma_ for token in doc]\n",
    "print(\"Lemmatized words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n",
      "John PERSON\n"
     ]
    }
   ],
   "source": [
    "# NER with spacy\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words: ['is', 'at', 'a', 'for', 'is', 'about', 'the']\n"
     ]
    }
   ],
   "source": [
    "# stop words in spacy\n",
    "stop_words = [token.text for token in doc if token.is_stop]\n",
    "print(\"Stop words:\", stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed words: ['appl', 'is', 'look', 'at', 'buy', 'a', 'u.k.', 'startup', 'for', '$', '1', 'billion', '.', 'john', 'is', 'excit', 'about', 'the', 'new', 'ventur', '.']\n"
     ]
    }
   ],
   "source": [
    "# using nktk stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(token.text) for token in nlp(text)]\n",
    "print(\"Stemmed words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['Apple', 'is', 'looking', 'at', 'buying', 'a', 'U.K.', 'startup', 'for', '$', '1', 'billion', '.', 'John', 'is', 'excited', 'about', 'the', 'new', 'venture', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words = word_tokenize(text)\n",
    "print(\"Words:\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature           | Stemming                               | Lemmatization                         |\n",
    "|-------------------|----------------------------------------|---------------------------------------|\n",
    "| **Definition**    |  Stemming is the process of reducing a word to its root or base form by chopping off prefixes and suffixes. This often results in words that may not be actual valid words in the language.| Lemmatization is a more sophisticated process that reduces a word to its lemmaâ€”the base or dictionary form of a word. It takes into account the wordâ€™s meaning and context.|\n",
    "| **Process**       | Chops off word endings based on rules  | Uses vocabulary and context to find base form |\n",
    "| **Output**        | Can result in non-real words (e.g., \"runn\") | Produces actual words (e.g., \"run\")  |\n",
    "| **Context-Awareness** | Not context-aware                    | Context-aware (considers POS)        |\n",
    "| **Accuracy**      | Less accurate, faster                  | More accurate, slightly slower       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities: [('Apple', 'GPE'), ('John', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "pos_tags = pos_tag(words)\n",
    "ner_tree = ne_chunk(pos_tags)\n",
    "named_entities = []\n",
    "for subtree in ner_tree:\n",
    "    if hasattr(subtree, \"label\"):\n",
    "        entity = \" \".join([leaf[0] for leaf in subtree.leaves()])\n",
    "        entity_type = subtree.label()\n",
    "        named_entities.append((entity, entity_type))\n",
    "\n",
    "print(\"Named Entities:\", named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words :['is', 'at', 'a', 'for', 'is', 'about', 'the']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words = [word for word in words if word.lower() in stop_words]\n",
    "print(f\"Stop words :{stop_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes : \n",
    "- When performing sentiment analysis, it is not recommended to remove all stop words, for example negation stopwords like not Ã¹ust be filtered and kept"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
