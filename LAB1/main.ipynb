{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 01 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.5'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model\n",
    "model = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.lang.en.English'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Google is planning to purchase a U.S. software company for $120 million.\"\n",
    "result = model(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Google is planning to purchase a U.S. software company for $120 million."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "print(type(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google | PROPN | nsubj |Google | False | False\n",
      "is | AUX | aux |be | True | False\n",
      "planning | VERB | ROOT |plan | False | False\n",
      "to | PART | aux |to | True | False\n",
      "purchase | VERB | xcomp |purchase | False | False\n",
      "a | DET | det |a | True | False\n",
      "U.S. | PROPN | compound |U.S. | False | False\n",
      "software | NOUN | compound |software | False | False\n",
      "company | NOUN | dobj |company | False | False\n",
      "for | ADP | prep |for | True | False\n",
      "$ | SYM | quantmod |$ | False | False\n",
      "120 | NUM | compound |120 | False | False\n",
      "million | NUM | pobj |million | False | False\n",
      ". | PUNCT | punct |. | False | True\n"
     ]
    }
   ],
   "source": [
    "for token in result:\n",
    "    print(\n",
    "        f\"{token.text} | {token.pos_} | {token.dep_} |{token.lemma_} | {token.is_stop} | {token.is_punct}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2) Five different properties of tokens that can be accessed using spaCy\n",
    "\n",
    "`text`: The original text of the token.\n",
    "\n",
    "Example: token.text returns \"Google\", \"is\", \"planning\", etc.\n",
    "\n",
    "`pos_` : The part-of-speech tag of the token.\n",
    "\n",
    "Example: token.pos_ might return \"NOUN\" for nouns, \"VERB\" for verbs.\n",
    "\n",
    "`dep_`: The syntactic dependency relation of the token (i.e., how the token relates to other tokens).\n",
    "\n",
    "Example: token.dep_ might return \"nsubj\" for a subject or \"dobj\" for a direct object.\n",
    "\n",
    "`lemma_`: The base form of the word.\n",
    "\n",
    "Example: token.lemma_ for the token \"planning\" returns \"plan\".\n",
    "\n",
    "`is_stop`: A boolean indicating whether the token is a stop word (e.g., \"the\", \"is\", \"and\").\n",
    "\n",
    "Example: token.is_stop returns True for common stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3) How does spaCy handle special cases in tokenization, such as punctuation, numbers, and abbreviations?\n",
    "\n",
    "`Punctuation:` SpaCy treats punctuation as separate tokens. For example, a period, comma, or quotation mark is treated as its own token.\n",
    "\n",
    "`Numbers:` Numbers are treated as single tokens. For example, \"$120\" would be tokenized as \"$\" and \"120\".\n",
    "\n",
    "`Abbreviations`: SpaCy usually handles abbreviations like \"U.S.\" correctly by keeping them as a single token instead of splitting them into multiple tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4) How does spaCy's tokenization differ from simple string splitting? Provide an example to illustrate the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text splitting : 12 tokens\n",
      "['Google', 'is', 'planning', 'to', 'purchase', 'a', 'U.S.', 'software', 'company', 'for', '$120', 'million.']\n",
      "====================\n",
      "Using Spacy : 14 tokens\n",
      "Google \n",
      "is \n",
      "planning \n",
      "to \n",
      "purchase \n",
      "a \n",
      "U.S. \n",
      "software \n",
      "company \n",
      "for \n",
      "$ \n",
      "120 \n",
      "million \n",
      ". \n"
     ]
    }
   ],
   "source": [
    "tokens1 = sent.split()\n",
    "tokens2 = model(sent)\n",
    "print(f\"Text splitting : {len(tokens1)} tokens\")\n",
    "print(tokens1)\n",
    "print(\"====================\")\n",
    "print(f\"Using Spacy : {len(tokens2)} tokens\")\n",
    "for token in tokens2:\n",
    "    print(f\"{token.text} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy splits the punctuation marks (e.g., the period after \"million\" and the \"$\" symbol) into separate tokens, while simple string splitting doesn't.\n",
    "Abbreviations like \"U.S.\" are handled better in spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5) Do the tokenization this time with word_tokenize from NLTK, what are the differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Google', 'is', 'planning', 'to', 'purchase', 'an', 'U.S.', 'software', 'company', 'for', '$', '120', 'million', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Google is planning to purchase an U.S. software company for $120 million.\"\n",
    "nltk_tokens = word_tokenize(text)\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both NLTK and spaCy split punctuation marks and numbers similarly in this case.\n",
    "The primary difference is that spaCy provides richer linguistic context (e.g., part-of-speech, dependency parsing), while NLTKâ€™s word_tokenize only splits the text into tokens without offering any additional linguistic analysis.\n",
    "SpaCy also handles special cases more robustly in many scenarios (e.g., handling of multi-word proper nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 02 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. Did he mind? Adam Jones Jr. thinks he didn't. In any case, this isn't true... Well, with a probability of .9 it isn't.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Sentence Segmentation:\n",
      "Number of Sentences: 1\n",
      "Sentence 1: Google is planning to purchase an U.S. software company for $120 million.\n",
      "\n",
      "\n",
      "NLTK Sentence Segmentation:\n",
      "Number of Sentences: 1\n",
      "Sentence 1: Google is planning to purchase an U.S. software company for $120 million.\n",
      "\n",
      "\n",
      "TextBlob Sentence Segmentation:\n",
      "Number of Sentences: 1\n",
      "Sentence 1: Google is planning to purchase an U.S. software company for $120 million.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "spacy_sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "print(\"spaCy Sentence Segmentation:\")\n",
    "print(f\"Number of Sentences: {len(spacy_sentences)}\")\n",
    "for idx, sent in enumerate(spacy_sentences):\n",
    "    print(f\"Sentence {idx+1}: {sent}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# 2. NLTK Approach\n",
    "nltk_sentences = sent_tokenize(text)\n",
    "\n",
    "print(\"NLTK Sentence Segmentation:\")\n",
    "print(f\"Number of Sentences: {len(nltk_sentences)}\")\n",
    "for idx, sent in enumerate(nltk_sentences):\n",
    "    print(f\"Sentence {idx+1}: {sent}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3. TextBlob Approach\n",
    "blob = TextBlob(text)\n",
    "textblob_sentences = [str(sentence) for sentence in blob.sentences]\n",
    "\n",
    "print(\"TextBlob Sentence Segmentation:\")\n",
    "print(f\"Number of Sentences: {len(textblob_sentences)}\")\n",
    "for idx, sent in enumerate(textblob_sentences):\n",
    "    print(f\"Sentence {idx+1}: {sent}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/wissem/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download(\"punkt\")\n",
    "text = \"Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. Did he mind? Adam Jones Jr. thinks he didn't. In any case, this isn't true... Well, with a probability of .9 it isn't.\"\n",
    "doc = nlp(text)\n",
    "spacy_sentences = [sent.text for sent in doc.sents]\n",
    "nltk_sentences = nltk.sent_tokenize(text)\n",
    "blob = TextBlob(text)\n",
    "textblob_sentences = [str(sentence) for sentence in blob.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Sentence Segmentation:\n",
      "- Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it.\n",
      "- Did he mind?\n",
      "- Adam Jones Jr. thinks he didn't.\n",
      "- In any case, this isn't true...\n",
      "- Well, with a probability of .9 it isn't.\n",
      "\n",
      "NLTK Sentence Segmentation:\n",
      "- Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e.\n",
      "- he paid a lot for it.\n",
      "- Did he mind?\n",
      "- Adam Jones Jr. thinks he didn't.\n",
      "- In any case, this isn't true... Well, with a probability of .9 it isn't.\n",
      "\n",
      "TextBlob Sentence Segmentation:\n",
      "- Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e.\n",
      "- he paid a lot for it.\n",
      "- Did he mind?\n",
      "- Adam Jones Jr. thinks he didn't.\n",
      "- In any case, this isn't true... Well, with a probability of .9 it isn't.\n"
     ]
    }
   ],
   "source": [
    "print(\"spaCy Sentence Segmentation:\")\n",
    "for sentence in spacy_sentences:\n",
    "    print(f\"- {sentence}\")\n",
    "\n",
    "print(\"\\nNLTK Sentence Segmentation:\")\n",
    "for sentence in nltk_sentences:\n",
    "    print(f\"- {sentence}\")\n",
    "\n",
    "print(\"\\nTextBlob Sentence Segmentation:\")\n",
    "for sentence in textblob_sentences:\n",
    "    print(f\"- {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Spacy` gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2) Compare the results of sentence segmentation from spaCy, NLTK, and TextBlob. Are there any differences in how they handle abbreviations, ellipsis, or other special cases?\n",
    "Comparison:\n",
    "\n",
    "spaCy: Handles abbreviations and ellipses well. For example, in the text, \"i.e.\" is correctly treated as part of the sentence and not as a sentence boundary. Similarly, ellipses like \"isn't true...\" do not trigger sentence splitting.\n",
    "NLTK: Sometimes struggles with abbreviations. It might split sentences after abbreviations like \"i.e.\" because it sees the period as an end-of-sentence marker.\n",
    "TextBlob: Similar to spaCy, it handles abbreviations and ellipses relatively well. It does not split sentences after \"i.e.\" and also manages ellipses like \"isn't true...\" without creating sentence boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
